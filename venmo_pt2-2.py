# -*- coding: utf-8 -*-
"""venmo pt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aVfoY4ZxyZpvw_zUvT6Jp0PL1CNlgBHA
"""

# old version
# Run below commands
# !apt-get install openjdk-8-jdk-headless -qq > /dev/null
# !wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
# !tar xf spark-3.1.2-bin-hadoop3.2.tgz
# !pip install -q findspark
# import os
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop3.2"
# versions should always align

!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz
!tar zxvf spark-3.4.0-bin-hadoop3.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.0-bin-hadoop3"
import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark import SparkConf, SparkContext

import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark import SparkConf, SparkContext
from datetime import datetime, date, timedelta
from dateutil import relativedelta
from pyspark.sql import SQLContext, Row
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql import DataFrame
from pyspark.sql.functions import *
from pyspark.sql.functions import to_timestamp, to_date
from pyspark.sql import functions as F
from pyspark.sql.functions import collect_list, collect_set, concat, first, array_distinct, col, size, expr
from pyspark.sql import DataFrame
import random
import pandas as pd
from pyspark.sql.functions import col

import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
import string
from textblob import TextBlob
import pandas as pd
from collections import Counter

spark = SparkSession\
        .builder\
        .getOrCreate()

import networkx as nx
from pyspark.sql import SparkSession

from google.colab import drive
drive.mount('/content/drive')



venmo_sample = spark.read.parquet('/content/drive/MyDrive/423 hw/VenmoSample.snappy.parquet')
venmo_sample.show(5)

venmo_sample.createOrReplaceTempView('venmo_sample')

"""**Question 5 Finding Friends and Friends of Friends**"""

# union the users
user_total = spark.sql('SELECT user1, user2, datetime FROM venmo_sample\
                        UNION\
                        SELECT user2, user1, datetime FROM venmo_sample')

user_total.show(100)

user_total.createOrReplaceTempView('user_total')

# find the friends of the users
user_f = spark.sql('SELECT user1 AS user, user2 AS friend, datetime \
                    FROM user_total \
                    ORDER BY user, datetime')

user_f.show(100)

user_f.createOrReplaceTempView('user_f')

# find the friends of friends of the users
user_fof = spark.sql('SELECT a1.user, a1.friend, a2.friend AS friends_of_friends\
                      FROM user_f AS a1\
                      INNER JOIN user_f AS a2\
                      ON a1.friend = a2.user\
                      WHERE a1.user <> a2.friend\
                      ORDER BY a1.user')

user_fof.show(100)

user_fof.createOrReplaceTempView('user_fof')

"""**Question 6 Social Network x Dynamic Analysis**"""

# find the registration date for each user

user_regi_date = spark.sql('SELECT user, min(datetime) AS registration_date\
                         FROM user_f\
                         GROUP BY user\
                         ORDER BY user')

user_regi_date.show(100)

user_regi_date.createOrReplaceTempView('user_regi_date')

# create the lifetime indicators

user_life = spark.sql('SELECT user_f.user, user_f.friend, user_f.datetime as transaction_datetime, user_regi_date.registration_date, ceil(months_between(transaction_datetime, user_regi_date.registration_date)) AS lifetime_indicator\
                        FROM user_f\
                        INNER JOIN user_regi_date\
                        ON user_f.user = user_regi_date.user\
                        ORDER BY user_f.user, transaction_datetime')

user_life.show(100)

user_life.createOrReplaceTempView('user_life')

"""**Question 6-1 Number of friend and FOF at each time point throughout the life time**"""

# calculate the number of friends and the number of fof for each user throughout the lifetime
user_ffof_count = spark.sql('SELECT\
                      distinct user_fof.user,\
                      COUNT(DISTINCT user_fof.friend) AS num_friends\
                      FROM user_fof\
                      INNER JOIN user_life on user_fof.user=user_life.user\
                      GROUP BY user_fof.user, user_life.lifetime_indicator\
                      ORDER BY user')
user_ffof_count.show(100)

user_ffof_count.createOrReplaceTempView('user_ffof_count')

# calculate the number of friends and the number of fof for each user during the first 12 months of lifetime
user_transaction_12 = spark.sql('SELECT *\
                      FROM user_life\
                      WHERE lifetime_indicator <= 12\
                      ORDER BY user')
user_transaction_12.show(100)

user_transaction_12.createOrReplaceTempView('user_transaction_12')



user_f_count_12 = spark.sql('SELECT\
                      distinct user_fof.user,\
                      COUNT(DISTINCT user_transaction_12.friend) AS num_friends,\
                      COUNT(DISTINCT user_fof.friends_of_friends) AS num_friends_of_friends\
                      FROM user_transaction_12\
                      INNER JOIN user_life on user_fof.user=user_life.user\
                      WHERE user_life.lifetime_indicator <= 12\
                      GROUP BY user_fof.user, user_life.lifetime_indicator\
                      ORDER BY user')
user_ffof_count.show(100)

user_ffof_count.createOrReplaceTempView('user_ffof_count')

"""**Question 6-2 Calculate clustering coefficients**"""

user_lifetime_ffof_each = spark.sql('SELECT user_life.user, user_life.lifetime_indicator, \
                                    COUNT(DISTINCT user_fof.friend) AS num_of_friends, \
                                    COUNT(DISTINCT user_fof.friends_of_friends) AS num_of_fof \
                                    FROM user_life \
                                    INNER JOIN user_fof ON user_life.user = user_fof.user \
                                    GROUP BY user_life.user, user_life.lifetime_indicator \
                                    ORDER BY user_life.user, user_life.lifetime_indicator')

user_lifetime_ffof_each.show(100)


user_lifetime_ffof_each.createOrReplaceTempView('user_lifetime_ffof_each')

user_lifetime_ffof = spark.sql('SELECT user_life.user,\
                               user_life.lifetime_indicator,\
                               COUNT(DISTINCT user_fof.friend) AS num_of_friends,\
                               SUM(num_of_friends) OVER (PARTITION BY user ORDER BY user_life.lifetime_indicator) AS total_friend_count,\
                               COUNT(DISTINCT user_fof.friends_of_friends) AS num_of_fof,\
                               SUM(num_of_fof) OVER (PARTITION BY user ORDER BY user_life.lifetime_indicator) AS total_fof_count \
                               FROM user_life \
                               INNER JOIN user_fof ON user_life.user = user_fof.user \
                               ORDER BY user_life.user, user_life.lifetime_indicator')

user_lifetime_ffof.show(100)

user_lifetime_ffof.createOrReplaceTempView('user_lifetime_ffof')















